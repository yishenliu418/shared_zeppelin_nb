{"paragraphs":[{"title":"download pareto jar","text":"%sh\naws s3 cp s3://economics-emr/jars/CoreAI-Pareto-lean-1.0.jar /dev/shm/CoreAI-Pareto-lean-1.0.jar","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Completed 256.0 KiB/2.7 MiB (1.3 MiB/s) with 1 file(s) remaining\rCompleted 512.0 KiB/2.7 MiB (2.6 MiB/s) with 1 file(s) remaining\rCompleted 768.0 KiB/2.7 MiB (3.8 MiB/s) with 1 file(s) remaining\rCompleted 1.0 MiB/2.7 MiB (4.9 MiB/s) with 1 file(s) remaining  \rCompleted 1.2 MiB/2.7 MiB (6.1 MiB/s) with 1 file(s) remaining  \rCompleted 1.5 MiB/2.7 MiB (7.2 MiB/s) with 1 file(s) remaining  \rCompleted 1.8 MiB/2.7 MiB (8.2 MiB/s) with 1 file(s) remaining  \rCompleted 2.0 MiB/2.7 MiB (9.3 MiB/s) with 1 file(s) remaining  \rCompleted 2.2 MiB/2.7 MiB (7.3 MiB/s) with 1 file(s) remaining  \rCompleted 2.5 MiB/2.7 MiB (8.0 MiB/s) with 1 file(s) remaining  \rCompleted 2.7 MiB/2.7 MiB (8.5 MiB/s) with 1 file(s) remaining  \rdownload: s3://economics-emr/jars/CoreAI-Pareto-lean-1.0.jar to ../../../../dev/shm/CoreAI-Pareto-lean-1.0.jar\n"}]},"apps":[],"jobName":"paragraph_1665624759987_881180259","id":"20211218-031545_295947510","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:116841"},{"text":"%spark.dep\n\nz.reset()\nz.load(\"/dev/shm/CoreAI-Pareto-lean-1.0.jar\")","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"res0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@2400651e\n"}]},"apps":[],"jobName":"paragraph_1665624759987_1024434738","id":"20211218-031644_1485514131","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116842"},{"text":"import spark.implicits._\r\nimport org.apache.spark.sql._\r\nimport org.apache.spark.sql.functions._\r\nimport org.apache.spark.sql.types._\r\nimport org.apache.spark.storage.StorageLevel\r\nimport org.apache.spark.sql.expressions.Window\r\n\r\nimport com.amazon.coreai.pareto.lib.dsi.{RfDrOutcomePredictor, \r\n                                         CalibratedRfDrTreatmentPropensity, \r\n                                         Quantilization, \r\n                                         DrTreatmentEffectCalculator,\r\n                                         MultiTargetVariableSelection,\r\n                                         WeightPopulationBasedBandwidthSelection,\r\n                                         WeightPopulationBasedBandwidthSelectionModel, \r\n                                         BandwidthEntry,\r\n                                         RfExtrapolation\r\n                                        }\r\nimport com.amazon.coreai.pareto.lib.clustering.{KMeansEstimator}\r\nimport com.amazon.coreai.pareto.lib.utils.{PersistMethod}\r\nimport com.amazon.coreai.pareto.lib.regression.{OLS}\r\n\r\nimport org.apache.spark.ml.evaluation.{RegressionEvaluator, BinaryClassificationEvaluator}\r\n\r\n//helper functions;\r\ndef writeAndRead (df:DataFrame, filename:String):DataFrame = {\r\n    val dataPath = s\"s3://ocean-yishenli/custom/temp/${filename}\"\r\n    df.repartition(180).write.mode(\"overwrite\").parquet(dataPath)\r\n    spark.read.parquet(dataPath)\r\n}\r\n\r\ndef loadCheckPoint(filename:String):DataFrame = {\r\n    val dataPath = s\"s3://ocean-yishenli/custom/temp/${filename}\"\r\n    spark.read.parquet(dataPath)\r\n}\r\n\r\ndef show_shape(df:DataFrame):Unit = {\r\n    println(s\"number of rows: ${df.count}, number of columns: ${df.columns.size}\")\r\n}","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import spark.implicits._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.sql.expressions.Window\nimport com.amazon.coreai.pareto.lib.dsi.{RfDrOutcomePredictor, CalibratedRfDrTreatmentPropensity, Quantilization, DrTreatmentEffectCalculator, MultiTargetVariableSelection, WeightPopulationBasedBandwidthSelection, WeightPopulationBasedBandwidthSelectionModel, BandwidthEntry, RfExtrapolation}\nimport com.amazon.coreai.pareto.lib.clustering.KMeansEstimator\nimport com.amazon.coreai.pareto.lib.utils.PersistMethod\nimport com.amazon.coreai.pareto.lib.regression.OLS\nimport org.apache.spark.ml.evaluation.{RegressionEvaluator, BinaryClassificationEvaluator}\nwriteAndRead: (df: o..."}]},"apps":[],"jobName":"paragraph_1665624759988_1931256781","id":"20211217-014500_1900135129","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116843"},{"text":"def estimateATEreg(\n                estimationVersion: String,\n                featureVersion: String,\n                outputPath: String,\n                treatmentKnots: String,\n                df: DataFrame,\n                markupFilter: Double, // estimate on markup within this threshold by abs value\n                regDepth: Int, // regression model tree depth\n                regTrees: Int, // regression model number of trees\n                numFolds: Int = 2, // cross-fitting by 2 folds\n                parts: Int = 360\n                ): Unit = {\n    \n    val controlCols = spark.read.parquet(\"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/featureList/v4/\")\n                           .select(\"feature\")\n                           .as[String]\n                           .collect.toArray\n    val midRule = \"-\"*50\n    val botRule = \"=\"*50\n    val controlSize = controlCols.size\n    println(s\"\"\"\n            $midRule\n            estimation version: $estimationVersion\n            feature version : $featureVersion\n            number of control columns: $controlSize\n            \n            >>>>>>>>>>>\n            model spec:\n            \n            regDepth: $regDepth\n            regTrees: $regTrees\n            \n            >>>>>>>>>>>\n            \n            $botRule\n            \"\"\")         \n    \n    def treatment_buckets(treatmentKnots:String): Seq[org.apache.spark.sql.Column] = {\n        val treat_spec = treatmentKnots.split(\";\").map{pair => \n                        val pairNumbers = pair.split(\", \").map(_.replace(\"(\", \"\").replace(\")\", \"\").trim).toArray\n                        (pairNumbers(0).toDouble, pairNumbers(1).toDouble)\n            }.zipWithIndex.map {pair => \n            val lower = pair._1._1\n            val upper = pair._1._2\n            val id = pair._2\n            when(col(\"fisher50_markup\") > lower && col(\"fisher50_markup\") <= upper, 1).otherwise(0).as(s\"treatment_$id\")\n        }\n        treat_spec\n    }\n\n    val df_est = df.select(Seq($\"*\") ++ treatment_buckets(treatmentKnots) :_*)\n                 .filter(abs(col(\"fisher50_markup\")) < markupFilter)\n    \n    // treatment summary stats;\n    val df_summary = df_est.select(df_est.columns.filter(_.startsWith(\"treatment_\")).map(col):_*).describe()\n                            .withColumn(\"markup\", lit(\"fisher50_markup\"))\n                            .filter($\"summary\" =!= \"min\")\n                            .filter($\"summary\" =!= \"max\")\n                            .filter($\"summary\" =!= \"stddev\")\n    df_summary.show(false)\n    \n    // estimation;\n    val primaryKeys: Seq[String] = Seq(\"cid\", \"hit_day\", \"asin\")\n    val outcomeCol = \"gms_daily_l360clip\"\n    val treatmentCols =  df_est.columns.filter(_.startsWith(\"treatment_\")).toArray\n    \n    val outcomePredictor = RfDrOutcomePredictor(\n                            treatmentCols= treatmentCols,\n                            controlCols = controlCols,\n                            outcomeCol = outcomeCol,\n                            primaryKeys = primaryKeys,\n                            maxDepth = regDepth,\n                            numTrees = regTrees,\n                            minInstancesPerNode = 1,\n                            numFolds = numFolds,\n                            seed = 42,\n                            persistMethod = PersistMethod.cache(level = StorageLevel.DISK_ONLY, repartition=parts))\n                            \n    val outcomePredResult = outcomePredictor.fit(df_est.na.fill(0)) \n    val outcomePredResultDF = outcomePredResult.transform\n    \n    // save estimation results\n    outcomePredResultDF.write.mode(\"overwrite\").parquet(outputPath + estimationVersion + \"/reg/\")\n    }","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"estimateATEreg: (estimationVersion: String, featureVersion: String, outputPath: String, treatmentKnots: String, df: org.apache.spark.sql.DataFrame, markupFilter: Double, regDepth: Int, regTrees: Int, numFolds: Int, parts: Int)Unit\n"}]},"apps":[],"jobName":"paragraph_1665624759988_-1451984707","id":"20221012-195220_1544147159","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116844"},{"text":"%sh\naws s3 ls s3://ocean-yishenli/custom/ltppi2/retail_model20220101/ate/","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"                           PRE v4/\n                           PRE v5/\n                           PRE v7/\n"}]},"apps":[],"jobName":"paragraph_1665624759988_-602785688","id":"20221012-195517_1848215000","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116845"},{"text":"val df = spark.read.parquet(\"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/estimationData/\")\n    \nval estimationVersion = \"spark2\"\nval featureVersion = \"v4\"\nval outputPath = \"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/ate/\"\nval treatmentKnots = \"(-0.1, 0.1); (0.1, 0.8); (-0.8, -0.1)\"\nval markupFilter = 0.8\nval regDepth = 18\nval regTrees = 45\n\nestimateATEreg(\n        estimationVersion = estimationVersion,\n        featureVersion = featureVersion,\n        outputPath = outputPath,\n        treatmentKnots = treatmentKnots,\n        df = df,\n        markupFilter = markupFilter, // estimate on markup within this threshold by abs value\n        regDepth = regDepth, // regression model tree depth\n        regTrees = regTrees // regression model number of trees\n)","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n            --------------------------------------------------\n            estimation version: spark2\n            feature version : v4\n            number of control columns: 1045\n            \n            >>>>>>>>>>>\n            model spec:\n            \n            regDepth: 18\n            regTrees: 45\n            \n            >>>>>>>>>>>\n            \n            ==================================================\n            \n+-------+-------------------+-------------------+------------------+---------------+\n|summary|treatment_0        |treatment_1        |treatment_2       |markup         |\n+-------+-------------------+-------------------+------------------+---------------+\n|count  |30134417           |30134417           |30134417          |fisher50_markup|\n|mean   |0.24158884507372416|0.25795667458905874|0.5004544803372171|fisher50_markup|\n+-------+-------------------+-------------------+------------------+---------------+\n\ndf: org.apache.spark.sql.DataFrame = [cid: decimal(24,0), hit_day: date ... 1049 more fields]\nestimationVersion: String = spark2\nfeatureVersion: String = v4\noutputPath: String = s3://ocean-yishenli/custom/ltppi2/retail_model20220101/ate/\ntreatmentKnots: String = (-0.1, 0.1); (0.1, 0.8); (-0.8, -0.1)\nmarkupFilter: Double = 0.8\nregDepth: Int = 18\nregTrees: Int = 45\n"}]},"apps":[],"jobName":"paragraph_1665624759988_1177870912","id":"20221012-195220_1476532618","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116846"},{"text":"%sh\naws s3 ls s3://ocean-yishenli/custom/ltppi2/retail_model20220101/ate/","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"                           PRE spark2/\n                           PRE v4/\n                           PRE v5/\n                           PRE v7/\n"}]},"apps":[],"jobName":"paragraph_1665624759988_-1009353339","id":"20221012-200757_2010960264","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116847"},{"text":"def estimateATEprop(\n                estimationVersion: String,\n                featureVersion: String,\n                outputPath: String,\n                propDepth: Int, // propensity model tree depth\n                propTrees: Int, // propensity model number of trees\n                numFolds: Int = 2, // cross-fitting by 2 folds\n                parts: Int = 360\n                ): Unit = {\n    \n    val controlCols = spark.read.parquet(\"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/featureList/v4/\")\n                           .select(\"feature\")\n                           .as[String]\n                           .collect.toArray\n    val midRule = \"-\"*50\n    val botRule = \"=\"*50\n    val controlSize = controlCols.size\n    println(s\"\"\"\n            $midRule\n            estimation version: $estimationVersion\n            feature version : $featureVersion\n            number of control columns: $controlSize\n            \n            >>>>>>>>>>>\n            model spec:\n            \n            propDepth: $propDepth\n            propTrees: $propTrees\n            \n            >>>>>>>>>>>\n            \n            $botRule\n            \"\"\")  \n        \n        val outcomePredResultDF = spark.read.parquet(\"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/ate/spark2/reg/\")\n        val primaryKeys: Seq[String] = Seq(\"cid\", \"hit_day\", \"asin\")\n        val outcomeCol = \"gms_daily_l360clip\"\n        val treatmentCols =  outcomePredResultDF.columns.filter(_.startsWith(\"treatment_\")).toArray\n            \n        val propensityPredictor = new CalibratedRfDrTreatmentPropensity(\n                                         treatmentCols = treatmentCols,\n                                         controlCols = controlCols,\n                                         primaryKeys = primaryKeys,\n                                         maxDepth = propDepth,\n                                         numTrees = propTrees,\n                                         minInstancesPerNode = 1,\n                                         numFolds = numFolds,\n                                         seed = 42,\n                                         persistMethod = PersistMethod.cache(level = StorageLevel.DISK_ONLY, repartition=parts)\n                                         )       \n        val propensityPredResult = propensityPredictor.fit(outcomePredResultDF)\n        val propensityPredResultDF = propensityPredResult.transform\n        println(\"propensity model is done.\")\n        \n        println(midRule)\n        println(\"ATE full:\")\n        DrTreatmentEffectCalculator.calculateAggregatedTreatmentEffect(\n                                    data = propensityPredResultDF, \n                                    treatmentCols = treatmentCols, \n                                    outcomeCol = outcomeCol,\n                                    eps = 0.0001).toDF.show(false)\n        println(botRule)\n    \n        // TE summary stats:\n        val df_te = DrTreatmentEffectCalculator.calculateItemLevelTreatmentEffect(\n                                        propensityPredResultDF, \n                                        treatmentCols = treatmentCols, \n                                        outcomeCol = outcomeCol,\n                                        eps = 0.0001)\n        val teCols = df_te.columns.filter(_.startsWith(\"te_treatment_\")).toArray                \n        df_te.select(teCols.map(col):_*)\n              .summary(\"count\", \"mean\",\"stddev\", \"min\", \"1%\", \"5%\", \"10%\", \"25%\", \"50%\", \"75%\", \"90%\", \"95%\", \"99%\", \"max\")\n              .show(false)\n        \n        // save estimation results\n        df_te.write.mode(\"overwrite\").parquet(outputPath + estimationVersion + \"/teDF/\")\n    }","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"estimateATEprop: (estimationVersion: String, featureVersion: String, outputPath: String, propDepth: Int, propTrees: Int, numFolds: Int, parts: Int)Unit\n"}]},"apps":[],"jobName":"paragraph_1665624759988_-838871370","id":"20221012-195220_1239620938","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116848"},{"text":"val estimationVersion = \"spark2\"\nval featureVersion = \"v4\"\nval outputPath = \"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/ate/\"\nval propDepth = 19\nval propTrees = 45\n\nestimateATEprop(\n        estimationVersion = estimationVersion,\n        featureVersion = featureVersion,\n        outputPath = outputPath,\n        propDepth = propDepth,  \n        propTrees = propTrees \n)","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n            --------------------------------------------------\n            estimation version: spark2\n            feature version : v4\n            number of control columns: 1045\n            \n            >>>>>>>>>>>\n            model spec:\n            \n            propDepth: 19\n            propTrees: 45\n            \n            >>>>>>>>>>>\n            \n            ==================================================\n            \npropensity model is done.\n--------------------------------------------------\nATE full:\n+-----------+------------------+------------------+------------------+----------------------+\n|treatment  |coefficient       |standardError     |tValue            |pValue                |\n+-----------+------------------+------------------+------------------+----------------------+\n|treatment_1|-6.965176052222497|0.8726524483849774|-7.981615206733204|1.5543122344752192E-15|\n|treatment_2|12.455159066936147|1.0974411590033597|11.349272773992995|0.0                   |\n+-----------+------------------+------------------+------------------+----------------------+\n\n==================================================\n+-------+-------------------+-------------------+\n|summary|te_treatment_1     |te_treatment_2     |\n+-------+-------------------+-------------------+\n|count  |30134417           |30134417           |\n|mean   |-6.965176052222489 |12.455159066936154 |\n|stddev |4790.410334627797  |6024.384025352914  |\n|min    |-135707.67485792178|-489683.97106235276|\n|1%     |-15714.872286014735|-16042.3476176281  |\n|5%     |-6499.2837822582715|-5672.925772597479 |\n|10%    |-3408.782241053599 |-3231.392618250828 |\n|25%    |-458.618215641269  |-1087.0271737260664|\n|50%    |2.7656838299085393 |-37.611566626548665|\n|75%    |480.7341863453198  |1005.1610601366165 |\n|90%    |3589.7470716163043 |3801.544494890888  |\n|95%    |6445.939428136013  |6197.13857829146   |\n|99%    |14536.664505756144 |13718.61706745234  |\n|max    |177254.26763440343 |682253.9727723068  |\n+-------+-------------------+-------------------+\n\nestimationVersion: String = spark2\nfeatureVersion: String = v4\noutputPath: String = s3://ocean-yishenli/custom/ltppi2/retail_model20220101/ate/\npropDepth: Int = 19\npropTrees: Int = 45\n"}]},"apps":[],"jobName":"paragraph_1665624759989_-1890933080","id":"20221012-195219_848241795","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116849"},{"text":"val df_ate = spark.read.parquet(\"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/ate/spark2/teDF\")\nval outcomeCol = \"gms_daily_l360clip\"\nval treatmentCols =  df_ate.columns.filter(_.startsWith(\"treatment_\")).toArray \nval df_result = DrTreatmentEffectCalculator.calculateAggregatedTreatmentEffect(\n                                    data = df_ate, \n                                    treatmentCols = treatmentCols, \n                                    outcomeCol = outcomeCol,\n                                    eps = 0.01).toDF\n                                    .withColumn(\"treatment\", when($\"treatment\" === \"treatment_1\", lit(\"ugv_ate\")).otherwise(\"scgv_ate\"))\n                                    .show(false)","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---------+------------------+------------------+------------------+----------------------+\n|treatment|coefficient       |standardError     |tValue            |pValue                |\n+---------+------------------+------------------+------------------+----------------------+\n|ugv_ate  |-5.625701896214282|0.8278721855394937|-6.795374931636602|1.0803358208022473E-11|\n|scgv_ate |6.044792233393401 |0.9435630464543711|6.406346937926331 |1.4904988354658144E-10|\n+---------+------------------+------------------+------------------+----------------------+\n\ndf_ate: org.apache.spark.sql.DataFrame = [cid: decimal(24,0), hit_day: date ... 1067 more fields]\noutcomeCol: String = gms_daily_l360clip\ntreatmentCols: Array[String] = Array(treatment_0, treatment_1, treatment_2)\ndf_result: Unit = ()\n"}]},"apps":[],"jobName":"paragraph_1665624759989_1013262867","id":"20221012-195219_543875650","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116850"},{"text":"def clusteringHTE(hteVersion: String,\n                       features: Array[String],\n                       numClusters: Int,\n                       outputPath: String\n      ): Unit = {\n      \n          val df_ate = spark.read.parquet(\"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/ate/spark2/teDF\")\n          println(\"ATE DF dim: \")\n          show_shape(df_ate)\n          \n          val quantilization = Quantilization(targetCols = features)\n          val quantilizationModel = quantilization.fit(df_ate)\n          val quantilizationDF = quantilizationModel.transform(df_ate)\n          val quantilizationCols = quantilization.renameCols(features)\n          println(\"localization features:\")\n          println(quantilizationCols)\n          println(\"quantilization is done.\")\n          println(\"-\"*100)\n          \n          val idCols = Array(\"cid\", \"hit_day\", \"asin\") \n          val outcomeCols = Array(\"gms_daily_l360clip\")\n          val treatmentCols = df_ate.columns.filter(_.contains(\"treatment\"))\n          val selectedFeatures = features.map(\"pct_\" + _)\n          val keepCols = idCols ++ outcomeCols ++ treatmentCols ++ selectedFeatures\n          quantilizationDF.select(keepCols.map(col):_*).repartition(180).write.mode(\"overwrite\").parquet(outputPath + hteVersion + \"/localDF/\")\n          println(\"scaling is done.\")\n          println(\"-\"*100)\n  \n          val clustering = KMeansEstimator(\n                             featureCols = selectedFeatures,\n                             primaryKeys = idCols,\n                             k = numClusters, // number of clusters\n                             standardizeFeatures = false, // default is true\n                             seed = 42,\n                             persistMethod= null\n              )\n          val clusteringSummary = clustering.fit(quantilizationDF)\n          val clusterCenters = clusteringSummary.clusterCenterEntries\n          val clusterPredictionDF = clusteringSummary.predictions.selectExpr(idCols ++ Array(\"prediction\"):_*)\n                                                     .withColumnRenamed(\"prediction\", \"centroid\")\n          // output clusterCenterEntries\n          clusterCenters.toDF.repartition(1).write.mode(\"overwrite\").parquet(outputPath + hteVersion + \"/clusterCenters/\")\n          // output cluster predictions;\n          clusteringSummary.predictions.write.mode(\"overwrite\").parquet(outputPath + hteVersion + \"/clusterPredictions/\")\n          println(\"clustering is done.\")\n          println(\"-\"*100)\n           \n    }","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"clusteringHTE: (hteVersion: String, features: Array[String], numClusters: Int, outputPath: String)Unit\n"}]},"apps":[],"jobName":"paragraph_1665624759989_1526864415","id":"20221012-202824_1239736731","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116851"},{"text":"val featureList = Array(\"brand_rank\", \"customer_review_count\", \"our_price\", \"ops_amzn\", \"sim_cnt\")\nval hteVersion = \"spark2\"\nval numClusters = 800\nval outputPath = \"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/hte/\"\nclusteringHTE(\n           hteVersion = hteVersion,\n           features = featureList,\n           numClusters = numClusters,\n           outputPath = outputPath\n           )","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"ATE DF dim: \nnumber of rows: 30134417, number of columns: 1069\nQuantilizing columns: brand_rank, customer_review_count, our_price, ops_amzn, sim_cnt\nlocalization features:\nArrayBuffer(pct_brand_rank, pct_customer_review_count, pct_our_price, pct_ops_amzn, pct_sim_cnt)\nquantilization is done.\n----------------------------------------------------------------------------------------------------\nscaling is done.\n----------------------------------------------------------------------------------------------------\nclustering is done.\n----------------------------------------------------------------------------------------------------\nfeatureList: Array[String] = Array(brand_rank, customer_review_count, our_price, ops_amzn, sim_cnt)\nhteVersion: String = spark2\nnumClusters: Int = 800\noutputPath: String = s3://ocean-yishenli/custom/ltppi2/retail_model20220101/hte/\n"}]},"apps":[],"jobName":"paragraph_1665624759989_1202643284","id":"20221012-202823_1923688117","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116852"},{"text":"import com.amazon.coreai.pareto.lib.clustering.ClusterCenterEntry\n\ndef getClusterCenterEntries(path:String): Seq[com.amazon.coreai.pareto.lib.clustering.ClusterCenterEntry] = {\n         val df_temp = spark.read.parquet(path)\n         df_temp.collect().map{row =>\n         val id = row.getString(0)\n         val values = row.getSeq[Double](1)\n         ClusterCenterEntry(id, values.toArray)\n        }.toSeq\n    }\n    \n    \ndef bandwidthHTE(): Unit = {\n    \n    val outputPath = \"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/hte/\"\n    val hteVersion = \"spark2\"\n    val clusterCenters = getClusterCenterEntries(outputPath + hteVersion + \"/clusterCenters/\")\n    // fix asin features for hte;\n    val featureList = Array(\"brand_rank\", \"customer_review_count\", \"our_price\", \"ops_amzn\", \"sim_cnt\")\n    val selectedFeatures = featureList.map(\"pct_\" + _)\n    val numberBandwidths = 11\n    val minPopulation = 0.1\n    val minWeight = 0.6\n    val maxScaling = 10\n    val localDF = spark.read.parquet(outputPath + hteVersion + \"/localDF/\")\n    val bandwidthSelection = WeightPopulationBasedBandwidthSelection(\n                                               clusterCenters = clusterCenters,\n                                               clusterFeatures = selectedFeatures,\n                                               threads = 60, \n                                               bandwidthChoices = numberBandwidths, \n                                               minPopulation = minPopulation, \n                                               minWeight = minWeight, \n                                               maxScaling = maxScaling,\n                                               persistMethod = PersistMethod.cache(level = StorageLevel.DISK_ONLY, repartition =180)\n                                               ).fit(localDF)\n    val bandwidths = bandwidthSelection.selectedBandwidths()  \n    // output bandwidth\n    bandwidths.toDF().repartition(60).write.mode(\"overwrite\").parquet(outputPath + hteVersion + \"/bandwidths/\")\n    println(\"bandwidth calculation is done.\")\n    println(\"-\"*100)\n     \n}","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import com.amazon.coreai.pareto.lib.clustering.ClusterCenterEntry\ngetClusterCenterEntries: (path: String)Seq[com.amazon.coreai.pareto.lib.clustering.ClusterCenterEntry]\nbandwidthHTE: ()Unit\n"}]},"apps":[],"jobName":"paragraph_1665624759989_-710654765","id":"20221012-202821_2140869352","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116853"},{"text":"bandwidthHTE()","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"bandwidth calculation is done.\n----------------------------------------------------------------------------------------------------\n"}]},"apps":[],"jobName":"paragraph_1665624759989_-691696642","id":"20221012-195218_439999166","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116854"},{"text":"import com.amazon.coreai.pareto.lib.dsi.BandwidthEntry\n\ndef getClusterCenterEntries(path:String): Seq[com.amazon.coreai.pareto.lib.clustering.ClusterCenterEntry] = {\n         val df_temp = spark.read.parquet(path)\n         df_temp.collect().map{row =>\n         val id = row.getString(0)\n         val values = row.getSeq[Double](1)\n         ClusterCenterEntry(id, values.toArray)\n        }.toSeq\n    }\n    \ndef getBandwidths(path:String) = {\n    val df_temp = spark.read.parquet(path)\n    df_temp.collect().map{row =>\n        val bId = row.getString(0)\n        val cId = row.getString(1)\n        val values = row.getSeq[Double](2)\n        val baseScaling = row.getAs[Int](3)\n        val additionalScaling = row.getAs[Int](4)\n        BandwidthEntry(bId, cId, values.toArray, baseScaling, additionalScaling)\n    }.toList\n}\n\n\ndef localizationHTE(\n                    numberTreatments: Int\n                    ): Unit = {\n    \n    val outputPath = \"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/hte/\"\n    val hteVersion = \"spark2\"\n    // load outcome from bandwidth step;                        \n    val clusterCenters = getClusterCenterEntries(outputPath + hteVersion + \"/clusterCenters/\")\n    val bandwidths = getBandwidths(outputPath + hteVersion + \"/bandwidths/\")\n    val localDF = spark.read.parquet(outputPath + hteVersion + \"/localDF/\")\n    // the order matters for treatmentCols below, first one is taken to be control group;\n    val treatmentColsOrdered = (0 to numberTreatments).map(num => s\"treatment_$num\")\n    // fix asin features for now;\n    val featureList = Array(\"brand_rank\", \"customer_review_count\", \"our_price\", \"ops_amzn\", \"sim_cnt\")\n    val selectedFeatures = featureList.map(\"pct_\" + _)\n    val clusterTreatmentEffects = DrTreatmentEffectCalculator.calculateClusterLevelTreatmentEffect(\n                                 data = localDF,  \n                                 treatmentCols = treatmentColsOrdered,\n                                 featureCols = selectedFeatures,\n                                 outcomeCol = \"gms_daily_l360clip\",\n                                 clusterCenters = clusterCenters,\n                                 bandwidths = bandwidths,\n                                 threads = 120,\n                                 eps = 0.0001,\n                                 persistMethod = PersistMethod.cache(level = StorageLevel.DISK_ONLY, repartition=180)\n                            )\n    // centroid level summary:\n    val windowSpec  = Window.partitionBy(\"clusterId\", \"treatment\").orderBy(\"bwInt\")\n    clusterTreatmentEffects.toDF\n                       .withColumn(\"bwInt\", col(\"bandwidthId\").cast(IntegerType))\n                       .withColumn(\"bw\", row_number.over(windowSpec))\n                       .groupBy(\"treatment\", \"bw\")\n                       .agg(count(\"clusterId\").as(\"volume\"),\n                            mean(\"coefficient\").as(\"ate\"),\n                            mean(\"standardError\").as(\"stderr\"))\n                       .orderBy(\"treatment\", \"bw\")\n                       .show(100, false)\n    clusterTreatmentEffects.toDF.write.mode(\"overwrite\").parquet(outputPath + hteVersion + \"/cteDF/\")\n    println(\"centroid localization is done.\")\n    println(\"-\"*100)    \n    \n    // double-check aggregate effect;\n    println(\"ATE summary:\")\n    val aggregateEffects = DrTreatmentEffectCalculator.calculateAggregatedTreatmentEffect(\n                                        data = localDF, \n                                        treatmentCols = treatmentColsOrdered, \n                                        outcomeCol = \"gms_daily_l360clip\",\n                                        eps = 0.0001)\n    aggregateEffects.toDF.show(false)\n    println(\"-\"*100)                        \n}","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import com.amazon.coreai.pareto.lib.dsi.BandwidthEntry\ngetClusterCenterEntries: (path: String)Seq[com.amazon.coreai.pareto.lib.clustering.ClusterCenterEntry]\ngetBandwidths: (path: String)List[com.amazon.coreai.pareto.lib.dsi.BandwidthEntry]\nlocalizationHTE: (numberTreatments: Int)Unit\n"}]},"apps":[],"jobName":"paragraph_1665624759989_-1080244418","id":"20221012-202857_1407075814","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116855"},{"text":"localizationHTE(2)","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------+---+------+-------------------+------------------+\n|treatment  |bw |volume|ate                |stderr            |\n+-----------+---+------+-------------------+------------------+\n|treatment_1|1  |800   |-8.105280723320233 |1.8348838182639846|\n|treatment_1|2  |800   |-7.99994331556366  |1.5788843155424002|\n|treatment_1|3  |800   |-7.891982202540941 |1.4066977350407677|\n|treatment_1|4  |800   |-7.792451546217375 |1.2858071154729678|\n|treatment_1|5  |800   |-7.704691268601748 |1.1983184348868587|\n|treatment_1|6  |800   |-7.628519491553524 |1.1335331101807287|\n|treatment_1|7  |800   |-7.562575112599211 |1.084659395454377 |\n|treatment_1|8  |800   |-7.505324634442882 |1.0471978395465924|\n|treatment_1|9  |800   |-7.4553970069484805|1.0180743290199432|\n|treatment_1|10 |800   |-7.4116490969303195|0.995139237498625 |\n|treatment_1|11 |800   |-7.37314487508008  |0.9768613568882003|\n|treatment_2|1  |800   |8.454919731950344  |1.9604964121063266|\n|treatment_2|2  |800   |8.486636304417628  |1.6966779879302267|\n|treatment_2|3  |800   |8.595194020708433  |1.5179142783918098|\n|treatment_2|4  |800   |8.772143088704336  |1.3929805948515894|\n|treatment_2|5  |800   |8.996424346854178  |1.3040806051706866|\n|treatment_2|6  |800   |9.245088244780739  |1.2401477094054307|\n|treatment_2|7  |800   |9.499486421440286  |1.1938814991882374|\n|treatment_2|8  |800   |9.74691052168791   |1.160279867408423 |\n|treatment_2|9  |800   |9.979874328665073  |1.1358338250453512|\n|treatment_2|10 |800   |10.194702079603722 |1.1180459364022663|\n|treatment_2|11 |800   |10.390204515064005 |1.1051223444709906|\n+-----------+---+------+-------------------+------------------+\n\ncentroid localization is done.\n----------------------------------------------------------------------------------------------------\nATE summary:\n+-----------+------------------+------------------+------------------+----------------------+\n|treatment  |coefficient       |standardError     |tValue            |pValue                |\n+-----------+------------------+------------------+------------------+----------------------+\n|treatment_1|-6.965176052222437|0.872652448384978 |-7.981615206733128|1.5543122344752192E-15|\n|treatment_2|12.45515906693615 |1.0974411590033577|11.349272773993016|0.0                   |\n+-----------+------------------+------------------+------------------+----------------------+\n\n----------------------------------------------------------------------------------------------------\n"}]},"apps":[],"jobName":"paragraph_1665624759990_96908205","id":"20221012-202857_2022721700","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116856"},{"text":"import com.amazon.coreai.pareto.lib.dsi.LocalizedTreatmentEffectEntry\n\ndef getClusterCenterEntries(path:String): Seq[com.amazon.coreai.pareto.lib.clustering.ClusterCenterEntry] = {\n         val df_temp = spark.read.parquet(path)\n         df_temp.collect().map{row =>\n         val id = row.getString(0)\n         val values = row.getSeq[Double](1)\n         ClusterCenterEntry(id, values.toArray)\n        }.toSeq\n    }\n    \ndef getBandwidths(path:String) = {\n    val df_temp = spark.read.parquet(path)\n    df_temp.collect().map{row =>\n        val bId = row.getString(0)\n        val cId = row.getString(1)\n        val values = row.getSeq[Double](2)\n        val baseScaling = row.getAs[Int](3)\n        val additionalScaling = row.getAs[Int](4)\n        BandwidthEntry(bId, cId, values.toArray, baseScaling, additionalScaling)\n    }.toList\n}\n\ndef getTeDF(path: String) = {\n    val df_temp = spark.read.parquet(path)\n    df_temp.collect().map{row =>\n        val clusterId = row.getString(0)\n        val bandwidthId = row.getString(1)\n        val treatment = row.getString(2)\n        val coefficient = row.getAs[Double](3)\n        val standardError = row.getAs[Double](4)\n        val tValue = row.getAs[Double](5)\n        val pValue = row.getAs[Double](6)\n        LocalizedTreatmentEffectEntry(clusterId, \n                                      bandwidthId, \n                                      treatment, \n                                      coefficient, \n                                      standardError,\n                                      tValue, \n                                      pValue)\n    }.toList\n}\n\ndef htePrediction(\n              numberBandwidths:Int,\n              numberTreatments:Int\n              ):Unit = {\n    \n    val outputPath = \"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/hte/\"\n    val hteVersion = \"spark2\"\n    \n    val teDF = getTeDF(outputPath + hteVersion + \"/cteDF/\") \n    val clusterCenters = getClusterCenterEntries(outputPath + hteVersion + \"/clusterCenters/\")\n    val bandwidths = getBandwidths(outputPath + hteVersion + \"/bandwidths/\")\n    val idCols = Array(\"cid\", \"hit_day\", \"asin\") \n    val clusterPredictionDF = spark.read.parquet(outputPath + hteVersion + \"/clusterPredictions/\")\n                                   .selectExpr(idCols ++ Array(\"prediction\"):_*)\n                                   .withColumnRenamed(\"prediction\", \"centroid\")\n    val localDF = spark.read.parquet(outputPath + hteVersion + \"/localDF/\")\n    val featureList = Array(\"brand_rank\", \"customer_review_count\", \"our_price\", \"ops_amzn\", \"sim_cnt\")\n    val selectedFeatures = featureList.map(\"pct_\" + _)\n    // output all bw results;\n    (0 to (numberBandwidths-1)).map{\n        i => {\n            println(s\"starting step: $i ...\")\n            val extrapolator = RfExtrapolation(\n                               // exclude control group here, because it only searches through clusterTreatmentEffects.treatment values;\n                               treatmentCols = (1 to numberTreatments).map(num => s\"treatment_$num\"),\n                               controlCols = selectedFeatures,\n                               treatmentEffects = teDF,\n                               clusterCenters = clusterCenters,\n                               bandwidths = bandwidths,\n                               bandwidthChoice = i,\n                               seed = 42,\n                               threads = 30)\n        val extrapolatorModel = extrapolator.fit()\n        val extrapolationDF = extrapolatorModel.transform(localDF)\n        val keepCols = idCols ++ extrapolationDF.columns.filter(_.endsWith(\"_hte\")).toSeq\n        val hteResult = extrapolationDF.select(keepCols.map(col):_*)\n                                   .join(clusterPredictionDF, idCols, \"inner\")\n                                   .withColumn(\"bw\", lit(i))\n        hteResult.write.mode(\"overwrite\").parquet(outputPath + hteVersion + s\"/hte_Bw$i\")\n        if (i == ((numberBandwidths+1)/2).toInt) {\n                hteResult.write.mode(\"overwrite\").parquet(outputPath + hteVersion + \"/hte_prediction/\")\n            }\n        }\n    }\n    println(\"all done.\")\n    println(\"=\"*100)\n}","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1665624759990_74169977","id":"20221012-204039_204074949","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116857"},{"text":"val numberBandwidths = 11\nval numberTreatments = 2\n\nhtePrediction(\n    numberBandwidths = numberBandwidths,\n    numberTreatments = numberTreatments\n    )","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"starting step: 0 ...\nstarting step: 1 ...\nstarting step: 2 ...\nstarting step: 3 ...\nstarting step: 4 ...\nstarting step: 5 ...\nstarting step: 6 ...\nstarting step: 7 ...\nstarting step: 8 ...\nstarting step: 9 ...\nstarting step: 10 ...\nall done.\n====================================================================================================\nnumberBandwidths: Int = 11\nnumberTreatments: Int = 2\n"}]},"apps":[],"jobName":"paragraph_1665624759990_-213184763","id":"20221012-204045_1658860956","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116858"},{"text":"def getClusterCenterEntries(path:String): Seq[com.amazon.coreai.pareto.lib.clustering.ClusterCenterEntry] = {\n         val df_temp = spark.read.parquet(path)\n         df_temp.collect().map{row =>\n         val id = row.getString(0)\n         val values = row.getSeq[Double](1)\n         ClusterCenterEntry(id, values.toArray)\n        }.toSeq\n    }\n    \ndef getBandwidths(path:String) = {\n    val df_temp = spark.read.parquet(path)\n    df_temp.collect().map{row =>\n        val bId = row.getString(0)\n        val cId = row.getString(1)\n        val values = row.getSeq[Double](2)\n        val baseScaling = row.getAs[Int](3)\n        val additionalScaling = row.getAs[Int](4)\n        BandwidthEntry(bId, cId, values.toArray, baseScaling, additionalScaling)\n    }.toList\n}\n\ndef getTeDF(path: String) = {\n    val df_temp = spark.read.parquet(path)\n    df_temp.collect.map{row =>\n        val clusterId = row.getString(0)\n        val bandwidthId = row.getString(1)\n        val treatment = row.getString(2)\n        val coefficient = row.getAs[Double](3)\n        val standardError = row.getAs[Double](4)\n        val tValue = row.getAs[Double](5)\n        val pValue = row.getAs[Double](6)\n        LocalizedTreatmentEffectEntry(clusterId, \n                                      bandwidthId, \n                                      treatment, \n                                      coefficient, \n                                      standardError,\n                                      tValue, \n                                      pValue)\n    }.toList\n}\n\ndef hteExtrapolationPB(\n              numberBandwidths:Int,\n              numberTreatments:Int\n              ):Unit = {\n    \n    val outputPath = \"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/hte/\"\n    val hteVersion = \"spark2\"\n    val atePath = \"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/ate/spark2/teDF/\"\n    val pdPath = \"s3://ocean-yishenli/custom/ltppi2/data/data/pb_extrapolation_data_v3.parquet\"\n    \n    // some variable names are not consistent between our estimation data vs pricing team provided PB dataset;\n    val df_pd = spark.read.parquet(pdPath)\n                     .withColumnRenamed(\"avg_our_price\", \"our_price\")\n                     .withColumnRenamed(\"ops\", \"ops_amzn\")\n                     .na.fill(0)\n    println(\"PB dataset dim:\")\n    show_shape(df_pd)\n    \n    val df_ate = spark.read.parquet(atePath)\n    println(\"ATE DF dim: \")\n    show_shape(df_ate)\n    \n    val featureList = Array(\"brand_rank\", \"customer_review_count\", \"our_price\", \"ops_amzn\", \"sim_cnt\")\n    val quantilization = Quantilization(targetCols = featureList)\n    val quantilizationModel = quantilization.fit(df_ate)\n    val quantilizationDF = quantilizationModel.transform(df_pd)\n    val quantilizationCols = quantilization.renameCols(featureList)\n    println(\"localization features:\")\n    println(quantilizationCols)\n    println(\"Quantilized PB dataset dim:\")\n    show_shape(quantilizationDF)\n    println(\"quantilization is done.\")\n    println(\"-\"*100)\n    \n    val teDF = getTeDF(outputPath + hteVersion + \"/cteDF/\") \n    val clusterCenters = getClusterCenterEntries(outputPath + hteVersion + \"/clusterCenters/\")\n    val bandwidths = getBandwidths(outputPath + hteVersion + \"/bandwidths/\")\n    val idCols = Array(\"asin\") \n    val selectedFeatures = featureList.map(\"pct_\" + _)\n    println(\"start extrapolating...\")\n    // output all bw results;\n    (0 to (numberBandwidths-1)).map{\n        i => {\n            println(s\"starting step: $i ...\")\n            val extrapolator = RfExtrapolation(\n                               // exclude control group here, because it only searches through clusterTreatmentEffects.treatment values;\n                               treatmentCols = (1 to numberTreatments).map(num => s\"treatment_$num\"),\n                               controlCols = selectedFeatures,\n                               treatmentEffects = teDF,\n                               clusterCenters = clusterCenters,\n                               bandwidths = bandwidths,\n                               bandwidthChoice = i,\n                               seed = 42,\n                               threads = 30)\n        val extrapolatorModel = extrapolator.fit()\n        println(s\"bandwidth $i is done.\")\n        val extrapolationDF = extrapolatorModel.transform(quantilizationDF)\n        val keepCols = idCols ++ extrapolationDF.columns.filter(_.endsWith(\"_hte\")).toSeq\n        val hteResult = extrapolationDF.select(keepCols.map(col):_*)\n                                   .withColumn(\"bw\", lit(i))\n        hteResult.write.mode(\"overwrite\").parquet(outputPath + hteVersion + s\"/pb_hte_Bw$i\")\n        if (i == ((numberBandwidths+1)/2).toInt) {\n                hteResult.write.mode(\"overwrite\").parquet(outputPath + hteVersion + \"/pb_hte_prediction/\")\n            }\n        }\n    }\n    println(\"all done.\")\n    println(\"=\"*100)\n}","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"getClusterCenterEntries: (path: String)Seq[com.amazon.coreai.pareto.lib.clustering.ClusterCenterEntry]\ngetBandwidths: (path: String)List[com.amazon.coreai.pareto.lib.dsi.BandwidthEntry]\ngetTeDF: (path: String)List[com.amazon.coreai.pareto.lib.dsi.LocalizedTreatmentEffectEntry]\nhteExtrapolationPB: (numberBandwidths: Int, numberTreatments: Int)Unit\n"}]},"apps":[],"jobName":"paragraph_1665624759990_-1657407445","id":"20221012-204045_1326076161","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116859"},{"text":"val numberBandwidths = 11\nval numberTreatments = 2\nhteExtrapolationPB(\n    numberBandwidths = numberBandwidths,\n    numberTreatments = numberTreatments\n    )","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"PB dataset dim:\nnumber of rows: 182533, number of columns: 14\nATE DF dim: \nnumber of rows: 30134417, number of columns: 1069\nQuantilizing columns: brand_rank, customer_review_count, our_price, ops_amzn, sim_cnt\nlocalization features:\nArrayBuffer(pct_brand_rank, pct_customer_review_count, pct_our_price, pct_ops_amzn, pct_sim_cnt)\nQuantilized PB dataset dim:\nnumber of rows: 182533, number of columns: 19\nquantilization is done.\n----------------------------------------------------------------------------------------------------\nstart extrapolating...\nstarting step: 0 ...\nbandwidth 0 is done.\nstarting step: 1 ...\nbandwidth 1 is done.\nstarting step: 2 ...\nbandwidth 2 is done.\nstarting step: 3 ...\nbandwidth 3 is done.\nstarting step: 4 ...\nbandwidth 4 is done.\nstarting step: 5 ...\nbandwidth 5 is done.\nstarting step: 6 ...\nbandwidth 6 is done.\nstarting step: 7 ...\nbandwidth 7 is done.\nstarting step: 8 ...\nbandwidth 8 is done.\nstarting step: 9 ...\nbandwidth 9 is done.\nstarting step: 10 ...\nbandwidth 10 is done.\nall done.\n====================================================================================================\nnumberBandwidths: Int = 11\nnumberTreatments: Int = 2\n"}]},"apps":[],"jobName":"paragraph_1665624759990_446318720","id":"20221012-204043_1117929090","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116860"},{"text":"val spark2_ate = \"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/ate/spark2/\"\nval spark3_ate = \"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/ate/v4/\"\nval spark2_hte = \"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/hte/spark2/\"\nval spark3_hte = \"s3://ocean-yishenli/custom/ltppi2/retail_model20220101/hte/v1/\"\n","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"spark2_ate: String = s3://ocean-yishenli/custom/ltppi2/retail_model20220101/ate/spark2/\nspark3_ate: String = s3://ocean-yishenli/custom/ltppi2/retail_model20220101/ate/v4/\nspark2_hte: String = s3://ocean-yishenli/custom/ltppi2/retail_model20220101/hte/spark2/\nspark3_hte: String = s3://ocean-yishenli/custom/ltppi2/retail_model20220101/hte/v1/\n"}]},"apps":[],"jobName":"paragraph_1665624759990_-1314279705","id":"20221012-204041_577736397","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116861"},{"text":"val result = \"ate result\"\nArray(spark2_ate, spark3_ate).map{path =>\n    if (path.contains(\"spark2\")) {\n        println(s\"spark 2 $result:\")\n    } else {\n        println(s\"spark 3 $result:\")\n    }\n    \n    val df = spark.read.parquet(path + \"teDF\")\n    val outcomeCol = \"gms_daily_l360clip\"\n    val treatmentCols =  df.columns.filter(_.startsWith(\"treatment_\")).toArray\n    DrTreatmentEffectCalculator.calculateAggregatedTreatmentEffect(\n                                        data = df, \n                                        treatmentCols = treatmentCols, \n                                        outcomeCol = outcomeCol,\n                                        eps = 0.0001).toDF.show(false)\n    println()\n}","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"spark 2 ate result:\n+-----------+------------------+------------------+------------------+----------------------+\n|treatment  |coefficient       |standardError     |tValue            |pValue                |\n+-----------+------------------+------------------+------------------+----------------------+\n|treatment_1|-6.96517605222254 |0.8726524483849823|-7.981615206733206|1.5543122344752192E-15|\n|treatment_2|12.455159066936114|1.097441159003359 |11.34927277399297 |0.0                   |\n+-----------+------------------+------------------+------------------+----------------------+\n\n\nspark 3 ate result:\n+-----------+------------------+------------------+------------------+--------------------+\n|treatment  |coefficient       |standardError     |tValue            |pValue              |\n+-----------+------------------+------------------+------------------+--------------------+\n|treatment_1|-3.518804569590335|1.2960584836844786|-2.715004464603294|0.006627491444996192|\n|treatment_2|2.6503711918599118|0.8964530587482256|2.9565086158117344|0.003111437778792858|\n+-----------+------------------+------------------+------------------+--------------------+\n\n\nresult: String = ate result\nres12: Array[Unit] = Array((), ())\n"}]},"apps":[],"jobName":"paragraph_1665624759991_230418020","id":"20221012-204222_766806327","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116862"},{"text":"val result = \"te result\"\nArray(spark2_ate, spark3_ate).map{path =>\n    if (path.contains(\"spark2\")) {\n        println(s\"spark 2 $result:\")\n    } else {\n        println(s\"spark 3 $result:\")\n    }\n    \n    val df = spark.read.parquet(path + \"teDF\")\n    val outcomeCol = \"gms_daily_l360clip\"\n    val treatmentCols =  df.columns.filter(_.startsWith(\"treatment_\")).toArray\n    val df_te = DrTreatmentEffectCalculator.calculateItemLevelTreatmentEffect(\n                                    df, \n                                    treatmentCols = treatmentCols, \n                                    outcomeCol = outcomeCol,\n                                    eps = 0.0001)\n    val teCols = df_te.columns.filter(_.startsWith(\"te_treatment_\")).toArray                \n    df_te.select(teCols.map(col):_*)\n          .summary(\"count\", \"mean\",\"stddev\", \"min\", \"1%\", \"5%\", \"10%\", \"25%\", \"50%\", \"75%\", \"90%\", \"95%\", \"99%\", \"max\")\n          .show(false)\n    println()\n}","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"colWidth":12,"fontSize":9,"results":{"0":{"graph":{"mode":"table","height":454,"optionOpen":false}}},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"spark 2 te result:\n+-------+-------------------+-------------------+\n|summary|te_treatment_1     |te_treatment_2     |\n+-------+-------------------+-------------------+\n|count  |30134417           |30134417           |\n|mean   |-6.9651760522225326|12.45515906693611  |\n|stddev |4790.410334627797  |6024.384025352915  |\n|min    |-135707.67485792178|-489683.97106235276|\n|1%     |-15647.414462022769|-15904.641993992765|\n|5%     |-6423.789913760713 |-5666.814719353927 |\n|10%    |-3378.233678262063 |-3185.69360455933  |\n|25%    |-456.71351262391215|-1086.5417643177889|\n|50%    |2.725677757948006  |-37.43915084154878 |\n|75%    |482.5661315332618  |1004.7537782506388 |\n|90%    |3588.6370583474577 |3803.6328671198844 |\n|95%    |6455.467620776879  |6197.434202987253  |\n|99%    |14635.881697262697 |13734.990677758102 |\n|max    |177254.26763440343 |682253.9727723068  |\n+-------+-------------------+-------------------+\n\n\nspark 3 te result:\n+-------+-------------------+-------------------+\n|summary|te_treatment_1     |te_treatment_2     |\n+-------+-------------------+-------------------+\n|count  |30134417           |30134417           |\n|mean   |-3.518804569590315 |2.650371191859913  |\n|stddev |7114.690351255553  |4921.063368450742  |\n|min    |-784043.5787752684 |-776699.1784292195 |\n|1%     |-13478.81455432853 |-11591.11737416786 |\n|5%     |-5519.575916677783 |-5005.05543739714  |\n|10%    |-2808.2430765352647|-2986.6044165346393|\n|25%    |-408.9396253048926 |-1004.0129179473151|\n|50%    |9.186836052209173  |-28.318421072614797|\n|75%    |495.75440160877315 |938.43010430812    |\n|90%    |3481.8911227462568 |3840.102802310559  |\n|95%    |7022.588340439712  |6235.278246359298  |\n|99%    |15820.72600279127  |14140.744621651378 |\n|max    |1355331.8068322875 |780161.3899783602  |\n+-------+-------------------+-------------------+\n\n\nresult: String = te result\nres13: Array[Unit] = Array((), ())\n"}]},"apps":[],"jobName":"paragraph_1665624759991_-1036505944","id":"20221012-204219_1574860647","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116863"},{"text":"val result = \"ate result with 0.01 eps\"\nArray(spark2_ate, spark3_ate).map{path =>\n    if (path.contains(\"spark2\")) {\n        println(s\"spark 2 $result:\")\n    } else {\n        println(s\"spark 3 $result:\")\n    }\n    \n    val df = spark.read.parquet(path + \"teDF\")\n    val outcomeCol = \"gms_daily_l360clip\"\n    val treatmentCols =  df.columns.filter(_.startsWith(\"treatment_\")).toArray\n    DrTreatmentEffectCalculator.calculateAggregatedTreatmentEffect(\n                                        data = df, \n                                        treatmentCols = treatmentCols, \n                                        outcomeCol = outcomeCol,\n                                        eps = 0.01).toDF.show(false)\n    println()\n}","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"colWidth":12,"fontSize":9,"results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"spark 2 ate result with 0.01 eps:\n+-----------+------------------+------------------+------------------+----------------------+\n|treatment  |coefficient       |standardError     |tValue            |pValue                |\n+-----------+------------------+------------------+------------------+----------------------+\n|treatment_1|-5.625701896214268|0.8278721855394944|-6.795374931636581|1.0803358208022473E-11|\n|treatment_2|6.044792233393397 |0.9435630464543707|6.40634693792633  |1.4904988354658144E-10|\n+-----------+------------------+------------------+------------------+----------------------+\n\n\nspark 3 ate result with 0.01 eps:\n+-----------+-------------------+------------------+-------------------+------------------+\n|treatment  |coefficient        |standardError     |tValue             |pValue            |\n+-----------+-------------------+------------------+-------------------+------------------+\n|treatment_1|-1.5212778123896948|1.051719164089592 |-1.4464677114698872|0.148046072359437 |\n|treatment_2|0.8942097029177483 |0.8167452180702517|1.0948453485047935 |0.2735844036405912|\n+-----------+-------------------+------------------+-------------------+------------------+\n\n\nresult: String = ate result with 0.01 eps\nres14: Array[Unit] = Array((), ())\n"}]},"apps":[],"jobName":"paragraph_1665624759991_-1729468612","id":"20221012-204218_489973454","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116864"},{"text":"val result = \"regression prediction\"\nArray(spark2_ate, spark3_ate).map{path =>\n    if (path.contains(\"spark2\")) {\n        println(s\"spark 2 $result:\")\n    } else {\n        println(s\"spark 3 $result:\")\n    }\n    \n    val df = spark.read.parquet(path + \"teDF\")\n    val outcomeCol = \"gms_daily_l360clip\"\n    val treatmentCols =  df.columns.filter(_.startsWith(\"treatment_\")).toArray\n    // check value prediction fit;\n    treatmentCols.foreach{treatmentName =>\n        val predColName = \"G_\" + treatmentName\n        val G_evaluator = new RegressionEvaluator()\n                         .setPredictionCol(predColName)\n                         .setLabelCol(\"gms_daily_l360clip\")\n                         .setMetricName(\"r2\")\n        val r2 = G_evaluator.evaluate(df.filter(col(treatmentName) === 1))\n        println(s\"${treatmentName} R-squared: ${r2}\")\n    }\n    \n    // reg prediction summary:\n    df.select(df.columns.filter(_.startsWith(\"G_treatment_\")).map(col):_*).summary().show(false)\n    println()\n}","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"spark 2 regression prediction:\ntreatment_0 R-squared: 0.8248914048297814\ntreatment_1 R-squared: 0.8237990461879969\ntreatment_2 R-squared: 0.8326906686878486\n+-------+------------------+------------------+------------------+\n|summary|G_treatment_0     |G_treatment_1     |G_treatment_2     |\n+-------+------------------+------------------+------------------+\n|count  |30134417          |30134417          |30134417          |\n|mean   |4480.392216701086 |4490.702032304281 |4457.432947179892 |\n|stddev |3398.6435636687374|3398.4241950449373|3403.4921944402286|\n|min    |0.9547032504304175|0.0               |0.0               |\n|25%    |1939.712361813689 |1953.2580511350602|1925.3044543126744|\n|50%    |3690.784381036082 |3700.741456475784 |3659.933302924978 |\n|75%    |6176.389080582879 |6202.812575326636 |6155.446250495112 |\n|max    |18515.59          |18515.59          |18515.59          |\n+-------+------------------+------------------+------------------+\n\n\nspark 3 regression prediction:\ntreatment_0 R-squared: 0.824224716147785\ntreatment_1 R-squared: 0.8231223133449241\ntreatment_2 R-squared: 0.8319997459585664\n+-------+------------------+------------------+------------------+\n|summary|G_treatment_0     |G_treatment_1     |G_treatment_2     |\n+-------+------------------+------------------+------------------+\n|count  |30134417          |30134417          |30134417          |\n|mean   |4480.453240958844 |4490.424645582764 |4457.58645367781  |\n|stddev |3399.3587917299   |3398.591803364055 |3402.932375043508 |\n|min    |0.0               |2.1334454365590245|0.0               |\n|25%    |1995.8526132586342|2008.459825531811 |1963.352763028326 |\n|50%    |3736.0107674976784|3750.6820542413743|3720.45107961518  |\n|75%    |6271.780382063212 |6281.2155369517395|6254.319156608636 |\n|max    |18515.590000000066|18515.590000000087|18515.590000000113|\n+-------+------------------+------------------+------------------+\n\n\nresult: String = regression prediction\nres15: Array[Unit] = Array((), ())\n"}]},"apps":[],"jobName":"paragraph_1665624759991_456880613","id":"20221013-010648_2076057492","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116865"},{"text":"val result = \"propensity prediction\"\nArray(spark2_ate, spark3_ate).map{path =>\n    if (path.contains(\"spark2\")) {\n        println(s\"spark 2 $result:\")\n    } else {\n        println(s\"spark 3 $result:\")\n    }\n    \n    val df = spark.read.parquet(path + \"teDF\")\n    val outcomeCol = \"gms_daily_l360clip\"\n    val treatmentCols =  df.columns.filter(_.startsWith(\"treatment_\")).toArray\n    // prop prediction summary:\n    df.select(df.columns.filter(_.startsWith(\"P_treatment_\")).map(col):_*).summary().show(false)\n    \n    // check prop prediction fit;\n    treatmentCols.foreach {treatmentName =>\n        val predColName = \"P_\" + treatmentName\n        val P_evaluator = new BinaryClassificationEvaluator()\n                              .setRawPredictionCol(predColName)\n                              .setLabelCol(treatmentName)\n                              .setMetricName(\"areaUnderROC\")\n        val auc = P_evaluator.evaluate(df)\n        println(s\"$treatmentName AUC is $auc\")\n    }\n    println()\n}","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"spark 2 propensity prediction:\n+-------+-------------------+-------------------+--------------------+\n|summary|P_treatment_0      |P_treatment_1      |P_treatment_2       |\n+-------+-------------------+-------------------+--------------------+\n|count  |30134417           |30134417           |30134417            |\n|mean   |0.2395124501635481 |0.2629224235270944 |0.49756512630936    |\n|stddev |0.11086326736532685|0.19164669521678673|0.22738254551716225 |\n|min    |0.1268845701047222 |0.07523819392488763|0.010559784150942365|\n|25%    |0.16399889609602444|0.11125073124600937|0.32098212981065344 |\n|50%    |0.2009157600518588 |0.18563617131141744|0.5577048873149735  |\n|75%    |0.27610916591360934|0.3628182642660257 |0.6982310395370352  |\n|max    |0.9000529083557213 |0.8587499580473641 |0.7827078559581125  |\n+-------+-------------------+-------------------+--------------------+\n\ntreatment_0 AUC is 0.709814304154758\ntreatment_1 AUC is 0.8004097356403366\ntreatment_2 AUC is 0.8091135230486504\n\nspark 3 propensity prediction:\n+-------+--------------------+---------------------+---------------------+\n|summary|P_treatment_0       |P_treatment_1        |P_treatment_2        |\n+-------+--------------------+---------------------+---------------------+\n|count  |30134417            |30134417             |30134417             |\n|mean   |0.23858588671899816 |0.2537705798693197   |0.5076435334116828   |\n|stddev |0.1400174612419967  |0.20201455150273234  |0.2732298174912484   |\n|min    |0.005838422619159527|0.0050838572778791075|0.0036117660823422994|\n|25%    |0.13638580972588668 |0.06805089327048099  |0.2723993909184268   |\n|50%    |0.2297427089305166  |0.22380861136675867  |0.4772628940953357   |\n|75%    |0.3248557236752106  |0.40756082369591656  |0.7673442248700018   |\n|max    |0.9766019441074286  |0.9856181967344941   |0.988773901816743    |\n+-------+--------------------+---------------------+---------------------+\n\ntreatment_0 AUC is 0.7445437510206744\ntreatment_1 AUC is 0.8253759997850847\ntreatment_2 AUC is 0.8253783631740256\n\nresult: String = propensity prediction\nres16: Array[Unit] = Array((), ())\n"}]},"apps":[],"jobName":"paragraph_1665624759991_-61869067","id":"20221013-010647_78705517","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116866"},{"text":"val result = \"hte prediction\"\nArray(spark2_hte, spark3_hte).map{path =>\n    if (path.contains(\"spark2\")) {\n        println(s\"spark 2 $result:\")\n    } else {\n        println(s\"spark 3 $result:\")\n    }\n    \n    val df = spark.read.parquet(path + \"hte_prediction\")\n    df\n     .select(\"treatment_1_hte\", \"treatment_2_hte\")\n     .withColumnRenamed(\"treatment_1_hte\", \"ugv_hte\")\n     .withColumnRenamed(\"treatment_2_hte\", \"scgv_hte\")\n     .summary(\"count\", \"mean\",\"stddev\", \"min\", \"1%\", \"5%\", \"10%\", \"25%\", \"50%\", \"75%\", \"90%\", \"95%\", \"99%\", \"max\")\n     .show(false)\n    println()\n}","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"spark 2 hte prediction:\n+-------+-------------------+-------------------+\n|summary|ugv_hte            |scgv_hte           |\n+-------+-------------------+-------------------+\n|count  |30134417           |30134417           |\n|mean   |-7.543673344915137 |9.570105205285405  |\n|stddev |3.6039311007600197 |11.106902731040933 |\n|min    |-15.040842355642557|-5.437593033038729 |\n|1%     |-14.559272176251866|-5.1220025633198984|\n|5%     |-13.744012787699731|-4.340873844358304 |\n|10%    |-12.549719661992812|-3.054091381293281 |\n|25%    |-10.145962969803275|0.5925971537749832 |\n|50%    |-7.7887474725010595|6.5942672318586    |\n|75%    |-4.460436433258491 |17.599812459259645 |\n|90%    |-2.4958027233989197|26.561504010525443 |\n|95%    |-1.9655689250307453|31.138301635190043 |\n|99%    |-1.527480879569493 |36.27709132852592  |\n|max    |-1.268090996204319 |40.48144008156974  |\n+-------+-------------------+-------------------+\n\n\nspark 3 hte prediction:\n+-------+-------------------+--------------------+\n|summary|ugv_hte            |scgv_hte            |\n+-------+-------------------+--------------------+\n|count  |30134417           |30134417            |\n|mean   |-4.086842026622485 |3.04424628545377    |\n|stddev |0.7561033430098677 |2.7810667294939684  |\n|min    |-5.785120063629354 |-1.1071680966988686 |\n|1%     |-5.59006230250725  |-0.8779746410839359 |\n|5%     |-5.3984973637414955|-0.5655810396030809 |\n|10%    |-5.104808619715542 |-0.20370805190527905|\n|25%    |-4.577238530550355 |0.7075762900838182  |\n|50%    |-4.109880090886169 |2.582033134953086   |\n|75%    |-3.6779838269841414|4.779738691651094   |\n|90%    |-2.935827002712811 |7.147337960475378   |\n|95%    |-2.7378691176539633|8.550434102032412   |\n|99%    |-2.473863968518689 |10.096593130916188  |\n|max    |-2.256273870886873 |11.362272685391556  |\n+-------+-------------------+--------------------+\n\n\nresult: String = hte prediction\nres18: Array[Unit] = Array((), ())\n"}]},"apps":[],"jobName":"paragraph_1665624759992_1752153826","id":"20221013-010646_473818969","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116867"},{"text":"val result = \"PB hte prediction\"\nArray(spark2_hte, spark3_hte).map{path =>\n    if (path.contains(\"spark2\")) {\n        println(s\"spark 2 $result:\")\n    } else {\n        println(s\"spark 3 $result:\")\n    }\n    \n    val df = spark.read.parquet(path + \"pb_hte_prediction\")\n    df\n     .select(\"treatment_1_hte\", \"treatment_2_hte\")\n     .withColumnRenamed(\"treatment_1_hte\", \"ugv_hte\")\n     .withColumnRenamed(\"treatment_2_hte\", \"scgv_hte\")\n     .summary(\"count\", \"mean\",\"stddev\", \"min\", \"1%\", \"5%\", \"10%\", \"25%\", \"50%\", \"75%\", \"90%\", \"95%\", \"99%\", \"max\")\n     .show(false)\n    println()\n}","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"spark 2 PB hte prediction:\n+-------+-------------------+------------------+\n|summary|ugv_hte            |scgv_hte          |\n+-------+-------------------+------------------+\n|count  |182533             |182533            |\n|mean   |-8.071887595957296 |16.650794696735975|\n|stddev |2.4163593614476935 |6.741828014364548 |\n|min    |-14.708360738333841|-5.637080828006883|\n|1%     |-12.74133261252113 |1.547167312711552 |\n|5%     |-11.744100624329151|6.8477595044274455|\n|10%    |-11.120555644971308|8.44816553727433  |\n|25%    |-9.91642699828452  |11.708385312176258|\n|50%    |-8.343689544452333 |16.306600702353627|\n|75%    |-6.085051612995636 |21.232804521325146|\n|90%    |-4.574788858249811 |25.37997062761376 |\n|95%    |-4.054166362011159 |28.529832681220036|\n|99%    |-3.2338466299563438|32.9454772680741  |\n|max    |-1.3968236764493709|35.29988951377627 |\n+-------+-------------------+------------------+\n\n\nspark 3 PB hte prediction:\n+-------+-------------------+-------------------+\n|summary|ugv_hte            |scgv_hte           |\n+-------+-------------------+-------------------+\n|count  |182533             |182533             |\n|mean   |-4.2725392054962406|4.91149654783823   |\n|stddev |0.4091013629319293 |2.054057908212588  |\n|min    |-5.709156599017673 |-1.0435928229043552|\n|1%     |-5.3083246997499085|1.1886448907521534 |\n|5%     |-5.001024033609512 |2.1000182819590756 |\n|10%    |-4.822144230700962 |2.48161984377506   |\n|25%    |-4.528186275985137 |3.310494716088928  |\n|50%    |-4.25129582461488  |4.660397508078253  |\n|75%    |-3.9886219430829994|6.2601404301920756 |\n|90%    |-3.7453437821120175|7.906292778296918  |\n|95%    |-3.6804078956968076|8.775858053396902  |\n|99%    |-3.5216553139511717|9.823931928294158  |\n|max    |-2.2451453528490424|11.150216944988347 |\n+-------+-------------------+-------------------+\n\n\nresult: String = PB hte prediction\nres19: Array[Unit] = Array((), ())\n"}]},"apps":[],"jobName":"paragraph_1665624759992_-1375823185","id":"20221013-010645_1999068027","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116868"},{"user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1665624759992_488181085","id":"paragraph_1663864644531_831375339","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116869"},{"user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1665624759992_-536933082","id":"paragraph_1663864644325_1295135745","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116870"},{"user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1665624759992_-112127329","id":"paragraph_1663864644111_1084584912","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116871"},{"user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1665624759992_20558690","id":"paragraph_1663864643837_133891793","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116872"},{"text":"%sh\n","user":"anonymous","dateUpdated":"2022-10-13T01:32:39+0000","config":{"editorSetting":{"language":"sh","editOnDblClick":false,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/sh","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1665624759992_-1492717730","id":"20220409-015512_1260324194","dateCreated":"2022-10-13T01:32:39+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116873"}],"name":"z038_retail_model_estimation_spark2","id":"2HEM8ANF8","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}